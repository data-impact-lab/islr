---
title: "ISLR-Chapter-3-Lab"
author: "Samuel Hansen"
date: 2017-06-29
output:
  github_document:
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  dpi = 400, 
  out.width = "100%", 
  error = FALSE
)
```

```{r}
#Libraries to be used.
library(tidyverse)
```

# 3.6 Lab: Linear Regression

## 3.6.1 Libraries

The `library()` function is used to load libraries, or groups of functions and 
`library()` data sets that are not included in the base R distribution. Basic 
functions that perform least squares linear regression and other simple analyses
come standard with the base distribution, but more exotic functions require 
additional libraries. Here we load the `MASS` package, which is a very large
collection of data sets and functions. We also load the `ISLR` package, which
includes the data sets associated with this book.

```{r}
library(MASS)
library(ISLR)
```

If you receive an error message when loading any of these libraries, it
likely indicates that the corresponding library has not yet been installed
on your system. Some libraries, such as `MASS`, come with R and do not need to
be separately installed on your computer. However, other packages, such as
`ISLR`, must be downloaded the first time they are used. This can be done 
directly from within R. For example, on a Windows system, select the 
`Install package` option under the `Packages` tab. After you select any mirror 
site, a list of available packages will appear. Simply select the package you 
wish to install and R will automatically download the package. Alternatively, 
this can be done at the R command line via `install.packages("ISLR")`. This 
installation only needs to be done the first time you use a package. However,
the `library()` function must be called each time you wish to use a given
package.

## 3.6.2 Simple Linear Regression

The `MASS` library contains the `Boston` data set, which records `medv` (median
house value) for 506 neighborhoods around Boston. We will seek to predict
medv using 13 predictors such as `rm` (average number of rooms per house),
`age` (average age of houses), and `lstat` (percent of households with low
socioeconomic status).

```{r}
names(Boston)
```

To find out more about the data set, we can type `?Boston`.
We will start by using the `lm()` function to fit a simple linear regression
model, with `medv` as the response and `lstat` as the predictor. The basic
syntax is `lm(y ∼ x, data)`, where `y` is the response, `x` is the predictor, 
and `data` is the data set in which these two variables are kept.

```{r, eval=FALSE}
 lm.fit = lm(medv ~ lstat)
```

The command causes an error because R does not know where to find
the variables `medv` and `lstat`. The next line tells R that the variables are
in `Boston`. If we attach `Boston`, the first line works fine because R now
recognizes the variables.

```{r}
lm.fit = lm(medv ~ lstat, data = Boston)
```

If we type `lm.fit`, some basic information about the model is output.
For more detailed information, we use `summary(lm.fit)`. This gives us p-values
and standard errors for the coefficients, as well as the $R^2$ statistic
and F-statistic for the model.

```{r}
lm.fit
```

```{r}
lm.fit %>% 
  summary()
```

We can use the `names()` function in order to find out what other pieces of 
information are stored in `lm.fit`. Although we can extract these quantities
by name — e.g. `lm.fit$coefficients` — it is safer to use the extractor
functions like `coef()` to access them.

```{r}
names(lm.fit)
```

```{r}
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can
use the `confint()` command.

```{r}
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and 
`predict()` prediction intervals for the prediction of `medv` for a given value
of `lstat`.

```{r}
new_data <- tibble(lstat = c(5, 10, 15))

predict(
  object = lm.fit, 
  newdata = new_data, 
  interval = "confidence"
)
```

```{r}
predict(
  object = lm.fit, 
  newdata = new_data, 
  interval = "prediction"
)
```

For instance, the 95% confidence interval associated with a lstat value of
10 is (24.47, 25.63), and the 95% prediction interval is (12.828, 37.28). As
expected, the confidence and prediction intervals are centered around the
same point (a predicted value of 25.05 for `medv` when `lstat` equals 10), but
the latter are substantially wider.

We will now plot `medv` and lstat along with the least squares regression
line using the `ggplot()` and `geom_smooth()` functions.

```{r}
Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm")
```

**NOTE: THIS TEXT NEEDS UPDATING**

*There is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue later in this lab. The `abline()` function can be used to draw any line, not just the least squares regression line. To draw a line with intercept `a` and slope `b`, we type `abline(a, b)`. Below we experiment with some additional settings for plotting lines and points. The `lwd = 3` command causes the width of the regression line to be increased by a factor of 3; this works for the `plot()` and `lines()` functions also. We can also use the pch option to create different plotting symbols.*

```{r}
Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(
    method = "lm", 
    size = 3
  )

Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(
    method = "lm", 
    size = 3,
    color = "red"
  )

Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point(color = "red")

Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point(
    color = "red",
    shape = 20
  )

Boston %>% 
  ggplot(mapping = aes(x = lstat, y = medv)) +
  geom_point(
    color = "red",
    shape = "+"
  )
  
tibble(
  x = 1:20,
  y = 1:20,
  shape = 1:20
) %>% 
  ggplot(mapping = aes(
    x = x, 
    y = y, 
    shape = shape)
  ) +
  geom_point() +
  scale_shape_identity()
```

**NOTE: THIS TEXT NEEDS UPDATING**

*Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the `plot` function directly to the output from `lm()`. In general, this command will produce one plot at a time, and hitting `Enter` will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the `par()` function, which tells R to split the display screen into separate panels so that multiple plots can be viewed  simultaneously. For example, `par(mfrow = c(2, 2))` divides the plotting region into a 2 × 2 grid of panels.*

**NOTE: THIS CODE NEEDS UPDATING (perhaps delete section)**

```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
plot(predict (lm.fit), residuals (lm.fit))
plot(predict (lm.fit), rstudent (lm.fit))
```

On the basis of the residual plots, there is some evidence of non-linearity.
Leverage statistics can be computed for any number of predictors using the
`hatvalues()` function.

```{r}
tibble(
  hat_value = lm.fit %>% hatvalues(),
  index = seq_along(hat_value)
) %>% 
  ggplot(mapping = aes(x = index, y = hat_value)) +
  geom_point()
```

```{r}
lm.fit %>% 
  hatvalues() %>% 
  which.max()
```

The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage
statistic. 


## 3.6.3 Multiple Linear Regression

In order to fit a multiple linear regression model using least squares, we
again use the `lm()` function. The syntax `lm(y ∼ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` 
function now outputs the regression coefficients for all the predictors.

```{r}
lm.fit = lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

The `Boston` data set contains 13 variables, and so it would be cumbersome
to have to type all of these in order to perform a regression using all of the
predictors. Instead, we can use the following short-hand:

```{r}
lm.fit = lm(medv ~., data = Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name
(type `?summary.lm` to see what is available). Hence `summary(lm.fit)$r.sq`
gives us the $R^2$, and `summary(lm.fit)$sigma` gives us the `RSE`. The `vif()` function, part of the `car` package, can be used to compute variance inflation
factors. Most VIF’s are low to moderate for this data. The `car` package is
not part of the base R installation so it must be downloaded the first time
you use it via the `install.packages` option in R.

```{r}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the variables but
one? For example, in the above regression output, `age` has a high p-value.
So we may wish to run a regression excluding this predictor. The following
syntax results in a regression using all predictors except `age`.

```{r}
lm.fit1 = lm(medv ~. -age, data = Boston)
summary(lm.fit1)
```

